<p>
    <strong>1. Classical (A priori) Definition of Probability: </strong>
    \[If \; an \; experiment \; results \; in \; a \; total \; of \; (m + n) \; outcomes \; which \; are
    \; equally \; likely \; and \; mutually \; exclusive \; with \; one \; another \; and \]
    \[if \; 'm' \; outcomes \; are \; favorable \; to \; an \; event \; 'A' \; while \; 'n' \; are \;
    unfavorable, \; then \; the \; probability \; of \; occurrence \; of \; the \; event \; 'A'\]
    \[= P(A) = m/(m+n)= \frac{n(A)}{n(S)}\]
    \[We \; say \; that \; odds \; in \; favour \; of \; 'A' \; are \; m : n, \; while \; odds \;
    against \; 'A' \; are \; n : m\]
    \[P(\bar A)=\frac{n}{m+n}=1-P(A)\]
    <strong>2. Different Properties of probability: </strong>
    \[A. \; Addition \; theorem \; of \; probability:\]
    \[P(A \cup B) = P(A) + P(B) - P(A \cap B)\]
    \[B. \; De Morgan's \; Laws:\]
    \[i. \; (A \cup B)^c= A^c \cap B^c \; \; \; \; ii. \; (A \cap B)^c= A^c \cup B^c\]
    \[C. \; Distributive \; Laws: \]
    \[i. \; A \cup (B \cap C)=(A \cup B) \cap (A \cup C) \; \; \; \; ii. \; A \cap (B \cup C)=(A \cap B)
    \cup (A \cap C)\]
    \[iii. \; P(A \; or \; B \; or \; C) = P(A) + P(B) + P(C)- P(A \cap B) - P(B \cap C) - P(C \cap A)+
    P(A \cap B \cap C)\]
    \[iv. \; P(at \; least \; two \; of \; A,B,C \; occur) = P(A \cap B) + P(B \cap C) + P(C \cap A) -
    2P(A \cap B \cap C)\]
    \[v. \; P(exactly \; two \; of \; A,B,C \; occur) = P(A \cap B) + P(B \cap C) + P(C \cap A) - 3P(A
    \cap B \cap C)\]
    \[vi. \; P(exactly \; one \; of \; A,B,c \; occur) = P(A) + P(B) + P(C)- 2P(A \cap B) - 2P(B \cap C)
    - 2P(C \cap A)+ 3P(A \cap B \cap C)\]
    <strong>3. Conditional Probability: </strong>
    \[P(A/B)= \frac{P(A \cap B)}{P(B)}\]
    <strong>4. Binomial Probability Theorem: </strong>
    \[If \; an \; experiment \; is \; such \; that \; the \; probability \; of \; success \; or \;
    failure \; does \; not \; change \; with \; trials, \; then \; the \; probability \; of \; getting
    \]
    \[exactly \; r \; success \; in \; n \; trials \; of \; an \; experiment \; is \; ^nC_r p^r \;
    q^n-r, \; where \; 'p' \; is \; the \; probability \; of \; a \; success \; and \; q \; is \; the \;
    probability \; of \; a \; failure. \]
    \[Note \; that \; p+q=1\]
    <strong>5. Expectation: </strong>
    \[If \; a \; value \; M_i \; is \; associated \; with \; a \; probability \; of \; p_i, \; then \;
    the \; expectation \; is \; given \; by \sum p_iM_i\]
    <strong>6. Total Probability Theorem: </strong>
    \[P(A)= \sum_{i=1}^n P(B_i). P(A/B_i)\]
    <strong>7. Bayes's Theorem:</strong>
    \[If \; an \; event \; A \; can \; occur \; with \; one \; of \; the \; n \; mutually \; exclusive
    \; and \; exhaustive \; events \; B_1, B_2,....,B_n \; and \]
    \[the \; probabilities \; P(A/B_1),P(A/B_2),.....,P(A/B_n) \; are \; known, \; then \; P(B_i/A)=
    \frac{P(B_i).P(A/B_i)}{\sum_{i=1}^n P(B_i).P(A/B_i)} B_1,B_2,B_3,.....,B_n\]
    \[A= (A \cap B_1) \cup (A \cap B_2) \cup (A \cap B_3) \cup ..... \cup (A \cap B_n)\]
    \[P(A)=P(A \cap B_1)+P(A \cap B_2)+P(A \cap B_3)+.....+P(A \cap B_n)= \sum_{i=1}^n P(A \cap B_i)\]
    <strong>8. Binomial Probability Distribution: </strong>
    \[A. \; Mean \; of \; any \; probability \; distribution \; of \; a \; random \; variable \; is \;
    given \; by:\]
    \[\mu = \frac{\sum P_i \; X_i}{\sum P_i}= \sum P_i \; X_i = np\]
    \[n= number \; of \; trials, \; \; \; p=probability \; of \; success \; in \; each \; probability,
    \; \; \; q=probability \; of \; failure\]
    \[B. \; Variance \; of \; a \; random \; variable \; is \; given \; by, \sigma^2 = \sum (X_i- \mu
    )^2. P_i= \sum P_i X_i^2- \mu ^2 = npq \]
</p>